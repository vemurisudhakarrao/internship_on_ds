-----------------------------------------------------------------------------------------------------------------------------------------------------
# Importing the libraries 

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
%matplotlib inline

# Ignore harmless warnings 

import warnings 
warnings.filterwarnings("ignore")

# Set to display all the columns in dataset

pd.set_option("display.max_columns", None)

# Import psql to run queries 

import pandasql as psql
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Load the Loan data

LoanData = pd.read_csv(r"D:\00 Datasets\Others\Data-01\loan_data.csv", header=0)

# Copy to back-up file

LoanData_bk = LoanData.copy()

# Display first 5 values

LoanData.head()
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Display the Loan data information

LoanData.info()
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Change the name of variable

LoanData = LoanData.rename(columns = {'not.fully.paid': 'NFPaid'}, inplace = False)

# Copy the file to backup

LoanData_bk2 = LoanData.copy()

# Display first 5 records

LoanData.head()
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Count the target or dependent variable by '0' & '1' and 
# their proportion (> 10 : 1, then the dataset is imbalance dataset)

Target_count = LoanData.NFPaid.value_counts()
print('Class 0:', Target_count[0])
print('Class 1:', Target_count[1])
print('Proportion:', round(Target_count[0] / Target_count[1], 2), ': 1')
print('Total loans Trans:', len(LoanData))
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Displaying Duplicate values with in Loan ataset, if avialble

LoanData_dup = LoanData[LoanData.duplicated(keep='last')]
LoanData_dup
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Count the missing values by each variable, if available

LoanData.isnull().sum()
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Apply dummy variable function on 'purpose' categorical variable

cat_cols = ['purpose']

LoanData = pd.get_dummies(LoanData,columns=cat_cols)

LoanData = pd.DataFrame(LoanData)

LoanData.head()
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Identify the independent and Target (dependent) variables

IndepVar = []
for col in LoanData.columns:
    if col != 'NFPaid':
        IndepVar.append(col)

TargetVar = 'NFPaid'

x = LoanData[IndepVar]
y = LoanData[TargetVar]
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Splitting the dataset into train and test 

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Display all the variable in the loan dataset

LoanData.columns
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Identify variable which are suppose to normilize the values

mm_cols = ['int.rate', 'installment', 'log.annual.inc', 'dti', 'fico', 'days.with.cr.line', 'revol.bal', 'revol.util']
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Scaling the features by using MinMaxScaler

from sklearn.preprocessing import MinMaxScaler

mmscaler = MinMaxScaler(feature_range=(0, 1))

x_train[mm_cols] = mmscaler.fit_transform(x_train[mm_cols])
x_train = pd.DataFrame(x_train)

x_test[mm_cols] = mmscaler.fit_transform(x_test[mm_cols])
x_test = pd.DataFrame(x_test)
-----------------------------------------------------------------------------------------------------------------------------------------------------
# To build the 'Multinominal Decision Tree' model with random sampling

from sklearn.ensemble import RandomForestClassifier

ModelRF = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2,
                                 min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', 
                                 max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, 
                                 n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, 
                                 ccp_alpha=0.0, max_samples=None)

# Train the model with train data 

ModelRF.fit(x_train,y_train)

# Predict the model with test data set

y_pred = ModelRF.predict(x_test)
y_pred_prob = ModelRF.predict_proba(x_test)

# Confusion matrix in sklearn

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# actual values

actual = y_test

# predicted values

predicted = y_pred

# confusion matrix

matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)
print('Confusion matrix : \n', matrix)

# outcome values order in sklearn

tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
print('Outcome values : \n', tp, fn, fp, tn)

# classification report for precision, recall f1-score and accuracy

C_Report = classification_report(actual,predicted,labels=[1,0])

print('Classification report : \n', C_Report)

# calculating the metrics

sensitivity = round(tp/(tp+fn), 3);
specificity = round(tn/(tn+fp), 3);
accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);
balanced_accuracy = round((sensitivity+specificity)/2, 3);
precision = round(tp/(tp+fp), 3);
f1Score = round((2*tp/(2*tp + fp + fn)), 3);

# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. 
# A model with a score of +1 is a perfect model and -1 is a poor model

from math import sqrt

mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)
MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)

print('Accuracy :', round(accuracy*100, 2),'%')
print('Precision :', round(precision*100, 2),'%')
print('Recall :', round(sensitivity*100,2), '%')
print('F1 Score :', f1Score)
print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )
print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')
print('MCC :', MCC)

# Area under ROC curve 

from sklearn.metrics import roc_curve, roc_auc_score

print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))

# ROC Curve

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, y_pred)
fpr, tpr, thresholds = roc_curve(y_test,ModelRF.predict_proba(x_test)[:,1])
plt.figure()
#--------------------------------------------------------------------
plt.plot(fpr, tpr, label= 'Classification Model' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show() 
print('-----------------------------------------------------------------------------------------------------')
-----------------------------------------------------------------------------------------------------------------------------------------------------
# To get feature importance

from matplotlib import pyplot

importance = ModelRF.feature_importances_

# Summarize feature importance

for i,v in enumerate(importance):
    print('Feature: %0d, Score: %.5f' % (i,v))
    
# Plot feature importance

pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()
-----------------------------------------------------------------------------------------------------------------------------------------------------
# Results

PredResults = pd.DataFrame({'NFPaid_A':y_test, 'NFPaid_P':y_pred})

# Merge two Dataframes on index of both the dataframes

TestDataResults = LoanData_bk2.merge(PredResults, left_index=True, right_index=True)
TestDataResults.sample(10)
-----------------------------------------------------------------------------------------------------------------------------------------------------