-----------------------------------------------------------------------------------------------------------------------------------------------
# Importing the libraries 

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt

# Ignore harmless warnings 

import warnings 
warnings.filterwarnings("ignore")

# Set to display all the columns in dataset

pd.set_option("display.max_columns", None)

# Import psql to run queries 

import pandasql as psql
-----------------------------------------------------------------------------------------------------------------------------------------------
# Load the 'Customer Churn Problem' data

CustChurn = pd.read_csv(r"D:\00 Datasets\Others\Data-00\TSData\Churn_Modelling.csv", header=0)

# Copy the file to back-up

CustChurn_bk = CustChurn.copy()

# Display first 5 rows in the dataset

CustChurn.head()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Check if there are duplicate rows in the dataset

CustChurn.duplicated().any()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Drop the variables which are not infulencing on target variable

CustChurn = CustChurn.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)
-----------------------------------------------------------------------------------------------------------------------------------------------
# Display the information of dataset

CustChurn.info()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Display the Geography varibles count

CustChurn['Geography'].value_counts()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Display the Gender varibale count

CustChurn['Gender'].value_counts()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Convert 'Geography' and 'Gender' to numerical format using one hot encoding

CustChurn = pd.get_dummies(CustChurn, columns=['Geography', 'Gender'])

# Display the first 5 records 

CustChurn.head()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Display the information of dataset after conversion of variables

CustChurn.info()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Display the target varibale count

CustChurn['Exited'].value_counts()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Convert the target data type into category variable

CustChurn['Exited'] = CustChurn['Exited'].astype("category")
-----------------------------------------------------------------------------------------------------------------------------------------------
# Identify the Independent and Target variables

IndepVar = []
for col in CustChurn.columns:
    if col != 'Exited':
        IndepVar.append(col)

TargetVar = 'Exited'

x = CustChurn[IndepVar]
y = CustChurn[TargetVar]
-----------------------------------------------------------------------------------------------------------------------------------------------
# Splitting the dataset into train and test 

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)
x_train.shape, x_test.shape, y_train.shape, y_test.shape
-----------------------------------------------------------------------------------------------------------------------------------------------
# Feature Scaling - Each independent variable is in different range. The process of transforming all the 
# features in the given data set to a fixed range is known as ‘Scaling’

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.fit_transform(x_test)
-----------------------------------------------------------------------------------------------------------------------------------------------
# To build the 'Decision Tree' model with random sampling

from sklearn.tree import DecisionTreeClassifier 

CustChurnDT = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features=None,
                                     max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,
                                     min_samples_leaf=1, min_samples_split=2,min_weight_fraction_leaf=0.0,
                                     random_state=None, splitter='best')

# Train the model with train data 

CustChurnDT.fit(x_train,y_train)

# Predict the model with test data set

y_pred = CustChurnDT.predict(x_test)
y_pred_prob = CustChurnDT.predict_proba(x_test)

# Confusion matrix in sklearn

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# actual values

actual = y_test

# predicted values

predicted = y_pred

# confusion matrix

matrix = confusion_matrix(actual,predicted, labels=[1,0],sample_weight=None, normalize=None)
print('Confusion matrix : \n', matrix)

# outcome values order in sklearn

tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)
print('Outcome values : \n', tp, fn, fp, tn)

# classification report for precision, recall f1-score and accuracy

C_Report = classification_report(actual,predicted,labels=[1,0])

print('Classification report : \n', C_Report)

# calculating the metrics

sensitivity = round(tp/(tp+fn), 3);
specificity = round(tn/(tn+fp), 3);
accuracy = round((tp+tn)/(tp+fp+tn+fn), 3);
balanced_accuracy = round((sensitivity+specificity)/2, 3);
precision = round(tp/(tp+fp), 3);
f1Score = round((2*tp/(2*tp + fp + fn)), 3);

# Matthews Correlation Coefficient (MCC). Range of values of MCC lie between -1 to +1. 
# A model with a score of +1 is a perfect model and -1 is a poor model

from math import sqrt

mx = (tp+fp) * (tp+fn) * (tn+fp) * (tn+fn)
MCC = round(((tp * tn) - (fp * fn)) / sqrt(mx), 3)

print('Accuracy :', round(accuracy*100, 2),'%')
print('Precision :', round(precision*100, 2),'%')
print('Recall :', round(sensitivity*100,2), '%')
print('F1 Score :', f1Score)
print('Specificity or True Negative Rate :', round(specificity*100,2), '%'  )
print('Balanced Accuracy :', round(balanced_accuracy*100, 2),'%')
print('MCC :', MCC)

# Area under ROC curve 

from sklearn.metrics import roc_curve, roc_auc_score

print('roc_auc_score:', round(roc_auc_score(y_test, y_pred), 3))

# ROC Curve

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, y_pred)
fpr, tpr, thresholds = roc_curve(y_test, CustChurnDT.predict_proba(x_test)[:,1])
plt.figure()
# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot(fpr, tpr, label= 'Classification Model' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show() 
print('-----------------------------------------------------------------------------------------------------')
-----------------------------------------------------------------------------------------------------------------------------------------------
# To get feature importance

from matplotlib import pyplot

importance = CustChurnDT.feature_importances_

# Summarize feature importance

for i,v in enumerate(importance):
    print('Feature: %0d, Score: %.5f' % (i,v))
    
# Plot feature importance

pyplot.bar([x for x in range(len(importance))], importance)
pyplot.show()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Display the data information

CustChurn.info()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Results

PredResults = pd.DataFrame({'Exited_A':y_test, 'Exited_P':y_pred})

# Merge two Dataframes on index of both the dataframes

TestDataResults = CustChurn_bk.merge(PredResults, left_index=True, right_index=True)

# Display the 10 records randomly

TestDataResults.sample(10)
-----------------------------------------------------------------------------------------------------------------------------------------------
del TestDataResults['Exited']
TestDataResults.head()
-----------------------------------------------------------------------------------------------------------------------------------------------
# Plot the decision tree

import matplotlib.pyplot as plt
from sklearn import tree

plt.figure(figsize=(20,5))
tree.plot_tree(CustChurnDT);
-----------------------------------------------------------------------------------------------------------------------------------------------
